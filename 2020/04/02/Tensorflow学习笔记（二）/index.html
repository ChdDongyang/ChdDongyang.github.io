<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Tensorflow学习笔记（二） | Duang~</title><meta name="description" content="Tensorflow学习笔记（二）"><meta name="author" content="dongyang"><meta name="copyright" content="dongyang"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Tensorflow学习笔记（二）"><meta name="twitter:description" content="Tensorflow学习笔记（二）"><meta name="twitter:image" content="http://chddongyang.github.io/img/post2.jpeg"><meta property="og:type" content="article"><meta property="og:title" content="Tensorflow学习笔记（二）"><meta property="og:url" content="http://chddongyang.github.io/2020/04/02/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"><meta property="og:site_name" content="Duang~"><meta property="og:description" content="Tensorflow学习笔记（二）"><meta property="og:image" content="http://chddongyang.github.io/img/post2.jpeg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      now = new Date();
      hour = now.getHours();
      isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://chddongyang.github.io/2020/04/02/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"><link rel="prev" title="Tensorflow学习笔记（三）" href="http://chddongyang.github.io/2020/05/10/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"><link rel="next" title="Tensorflow学习笔记（一）" href="http://chddongyang.github.io/2020/03/23/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: false,
  isFontAwesomeV5: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">3</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">1</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">1</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensorflow学习笔记（二）"><span class="toc-text">Tensorflow学习笔记（二）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#鸢尾花数据集分类"><span class="toc-text">鸢尾花数据集分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#数据集介绍"><span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据集读入"><span class="toc-text">数据集读入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据集乱序"><span class="toc-text">数据集乱序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#分割训练集与测试集"><span class="toc-text">分割训练集与测试集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#将数据配对成-输入特征，标签-，每次喂入一小批-batch"><span class="toc-text">将数据配对成[输入特征，标签]，每次喂入一小批(batch)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义神经网络中所有可训练参数"><span class="toc-text">定义神经网络中所有可训练参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#嵌套循环迭代，with-结构更新参数，显示当前-loss"><span class="toc-text">嵌套循环迭代，with 结构更新参数，显示当前 loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#计算当前参数前向传播后的准确率，显示当前准确率-acc"><span class="toc-text">计算当前参数前向传播后的准确率，显示当前准确率 acc</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#acc-loss-可视化"><span class="toc-text">acc &#x2F; loss 可视化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#程序所有代码"><span class="toc-text">程序所有代码</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/post2.jpeg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Duang~</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Tensorflow学习笔记（二）</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-02<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-05-10</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Tensorflow2-1%E5%AE%9E%E8%B7%B5/">Tensorflow2.1实践</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h1 id="Tensorflow学习笔记（二）"><a href="#Tensorflow学习笔记（二）" class="headerlink" title="Tensorflow学习笔记（二）"></a>Tensorflow学习笔记（二）</h1><h2 id="鸢尾花数据集分类"><a href="#鸢尾花数据集分类" class="headerlink" title="鸢尾花数据集分类"></a>鸢尾花数据集分类</h2><p>对于机器学习初学者，鸢尾花分类可以说是非常好的入门项目了。虽然这个问题相对比较简单，但是其体现的神经网络的运行过程却很值得我们学习。对于多分类问题我们可以利用keras提供的接口来比较容易地实现，但是它并没有体现一些细节。本文利用tensorflow的一些基础函数来搭建神经网络，并体现神经网络是如何进行学习的，包括实现反向传播，前向传播，可视化损失函数等。</p>
<h4 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h4><p>本文采用sklearn.datasets包提供给我们的数据集，其提供了150组鸢尾花数据，每组包括鸢尾花的花萼长、花萼宽、花瓣长、花瓣宽4个输入特征，同时还给出了这一组特征对应的鸢尾花类别。类别包括狗尾鸢尾、杂色鸢尾、弗吉尼亚鸢尾三类， 分别用数字0、1、2 表示。</p>
<h4 id="数据集读入"><a href="#数据集读入" class="headerlink" title="数据集读入"></a>数据集读入</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br></pre></td></tr></table></figure>
<h4 id="数据集乱序"><a href="#数据集乱序" class="headerlink" title="数据集乱序"></a>数据集乱序</h4><p>随机打乱数据，因为原始数据是顺序的，打乱顺序有利于提高准确率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed，保证输入特征和标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br></pre></td></tr></table></figure></p>
<h4 id="分割训练集与测试集"><a href="#分割训练集与测试集" class="headerlink" title="分割训练集与测试集"></a>分割训练集与测试集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span></span><br><span class="line">x_train = x_data[:<span class="number">-30</span>]</span><br><span class="line">y_train = y_data[:<span class="number">-30</span>]</span><br><span class="line">x_test = x_data[<span class="number">-30</span>:]</span><br><span class="line">y_test = y_data[<span class="number">-30</span>:]</span><br></pre></td></tr></table></figure>
<h4 id="将数据配对成-输入特征，标签-，每次喂入一小批-batch"><a href="#将数据配对成-输入特征，标签-，每次喂入一小批-batch" class="headerlink" title="将数据配对成[输入特征，标签]，每次喂入一小批(batch)"></a>将数据配对成[输入特征，标签]，每次喂入一小批(batch)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义神经网络中所有可训练参数"><a href="#定义神经网络中所有可训练参数" class="headerlink" title="定义神经网络中所有可训练参数"></a>定义神经网络中所有可训练参数</h4><p>我们只用了一层网络，因为输入特征是4个，输出节点数等于分类数，是3分类，故参数w1为4行3列的张量，b1必须与w1的维度一致，所以是3。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>))</span><br></pre></td></tr></table></figure></p>
<h4 id="嵌套循环迭代，with-结构更新参数，显示当前-loss"><a href="#嵌套循环迭代，with-结构更新参数，显示当前-loss" class="headerlink" title="嵌套循环迭代，with 结构更新参数，显示当前 loss"></a>嵌套循环迭代，with 结构更新参数，显示当前 loss</h4><p>用两层 for 循环进行更新参数：第一层for循环是针对整个数据集进行循环，故用epoch表示；第二层for循环是针对batch的，用step表示。在with结构中计算前向传播的预测结果 y ，计算损失函数 loss损失函数loss，分别对参数w1和参数b1计算偏导数，更新参数w1和参数b1的值，打印出这一轮epoch 后的损失函数值。因为训练集有 120 组数据，batch 是 32，每个 step 只能喂入 32 组数据，需要batch级别循环4次，所以loss除以4，求得每次step 迭代的平均 loss。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> enumerate(train_db): <span class="comment">#batch 级别迭代</span></span><br><span class="line"> <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># 记录梯度信息</span></span><br><span class="line"> (前向传播过程计算 y)</span><br><span class="line"> (计算总 loss)</span><br><span class="line"> grads = tape.gradient(loss, [ w1, b1 ])</span><br><span class="line"> w1.assign_sub(lr * grads[<span class="number">0</span>]) <span class="comment">#参数自更新</span></span><br><span class="line"> b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line"> print(<span class="string">"Epoch &#123;&#125;, loss: &#123;&#125;"</span>.format(epoch, loss_all/<span class="number">4</span>))</span><br></pre></td></tr></table></figure></p>
<h4 id="计算当前参数前向传播后的准确率，显示当前准确率-acc"><a href="#计算当前参数前向传播后的准确率，显示当前准确率-acc" class="headerlink" title="计算当前参数前向传播后的准确率，显示当前准确率 acc"></a>计算当前参数前向传播后的准确率，显示当前准确率 acc</h4><p>前向传播计算出 y ，使其符合概率分布并找到最大的概率值对应的索引号，调整数据类型与标签一致，如果预测值和标签相等则correct变量自加一，准确率即预测对了的数量除以测试集中的数据总数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line"> y = tf.matmul(h, w) + b <span class="comment"># y 为预测结果</span></span><br><span class="line"> y = tf.nn.softmax(y) <span class="comment"># y 符合概率分布</span></span><br><span class="line"> pred = tf.argmax(y, axis=<span class="number">1</span>) <span class="comment"># 返回 y 中最大值的索引即预测的分类</span></span><br><span class="line"> pred = tf.cast(pred, dtype=y_test.dtype) <span class="comment"># 调整数据类型与标签一致</span></span><br><span class="line"> correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line"> correct = tf.reduce_sum (correct) <span class="comment"># 将每个 batch 的 correct 数加起来</span></span><br><span class="line"> total_correct += int (correct) <span class="comment"># 将所有 batch 中的 correct 数加起来</span></span><br><span class="line"> total_number += x_test.shape [<span class="number">0</span>]</span><br><span class="line">acc = total_correct / total_number</span><br><span class="line">print(<span class="string">"test_acc:"</span>, acc)</span><br></pre></td></tr></table></figure></p>
<h4 id="acc-loss-可视化"><a href="#acc-loss-可视化" class="headerlink" title="acc / loss 可视化"></a>acc / loss 可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.title(Acc Curve) <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(Epoch) <span class="comment"># x 轴名称</span></span><br><span class="line">plt.ylabel(Acc) <span class="comment"># y 轴名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">"$Accuracy$"</span>) <span class="comment"># 逐点画出 test_acc 值并连线</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="程序所有代码"><a href="#程序所有代码" class="headerlink" title="程序所有代码"></a>程序所有代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="comment"># 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"><span class="comment">#x_data = DataFrame(x_data,columns=['花萼长度','花萼宽度','花瓣长度','花瓣宽度'])</span></span><br><span class="line"><span class="comment">#pd.set_option('display.unicode.east_asian_width',True)#设置列名对齐</span></span><br><span class="line"><span class="comment">#print(x_data)</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed，保证输入特征和标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span></span><br><span class="line">x_train = x_data[:<span class="number">-30</span>]</span><br><span class="line">y_train = y_data[:<span class="number">-30</span>]</span><br><span class="line">x_test = x_data[<span class="number">-30</span>:]</span><br><span class="line">y_test = y_data[<span class="number">-30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()标记参数可训练</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率为0.1</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span></span><br><span class="line">test_acc = []  <span class="comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 循环500轮</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epoch):  <span class="comment">#数据集级别的循环，每个epoch循环一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> enumerate(train_db):  <span class="comment">#batch级别的循环 ，每个step循环一个batch</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_all += loss.numpy()  <span class="comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad</span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])  <span class="comment"># 参数w1自更新</span></span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])  <span class="comment"># 参数b自更新</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 每个epoch，打印loss信息</span></span><br><span class="line">    print(<span class="string">"Epoch &#123;&#125;, loss: &#123;&#125;"</span>.format(epoch, loss_all/<span class="number">4</span>))</span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)  <span class="comment"># 将4个step的loss求平均记录在此变量中</span></span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># loss_all归零，为记录下一个epoch的loss做准备</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 测试部分</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数进行预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引，即预测的分类</span></span><br><span class="line">        <span class="comment"># 将pred转换为y_test的数据类型</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)</span><br><span class="line">        <span class="comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)</span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += int(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 总的准确率等于total_correct/total_number</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    print(<span class="string">"Test_acc:"</span>, acc)</span><br><span class="line">    print(<span class="string">"--------------------------"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 绘制 loss 曲线</span></span><br><span class="line">plt.title(<span class="string">'Loss Function Curve'</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">"$Loss$"</span>)  <span class="comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线图标</span></span><br><span class="line">plt.show()  <span class="comment"># 画出图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 Accuracy 曲线</span></span><br><span class="line">plt.title(<span class="string">'Acc Curve'</span>)  <span class="comment"># 图片标题</span></span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)  <span class="comment"># x轴变量名称</span></span><br><span class="line">plt.ylabel(<span class="string">'Acc'</span>)  <span class="comment"># y轴变量名称</span></span><br><span class="line">plt.plot(test_acc, label=<span class="string">"$Accuracy$"</span>)  <span class="comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果</p>
</blockquote>
<p><img src="/" alt="" class="lazyload" data-src="loss.png"><br><img src="/" alt="" class="lazyload" data-src="acc.png"></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">dongyang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://chddongyang.github.io/2020/04/02/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/">http://chddongyang.github.io/2020/04/02/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://chddongyang.github.io" target="_blank">Duang~</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/post1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付宝"/><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/05/10/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"><img class="prev_cover lazyload" data-src="/img/post1.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Tensorflow学习笔记（三）</div></div></a></div><div class="next-post pull_right"><a href="/2020/03/23/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"><img class="next_cover lazyload" data-src="/img/post3.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Tensorflow学习笔记（一）</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By dongyang</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script defer id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/third-party/click_heart.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"live2d-widget-model-shizuku"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>